{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "script0707",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWW2z8WE2eMG"
      },
      "source": [
        "# ================================================================================ #\n",
        "# =========================== Goolge Colab File Upload =========================== #\n",
        "# ================================================================================ #\n",
        "\n",
        "#from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import output\n",
        "# !cp 파일1 파일2 # 파일1을 파일2로 복사 붙여넣기\n",
        "!cp \"/content/drive/MyDrive/Colab Notebooks/뉴스 토픽 분류 경진대회/open2.zip\" \"open2.zip\"\n",
        "# data.zip을 현재 디렉터리에 압축해제\n",
        "!unzip \"open2.zip\"\n",
        "output.clear()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exV0dTQE3sAj"
      },
      "source": [
        "# import library \n",
        "import numpy as np\n",
        "from numpy.lib.function_base import _cov_dispatcher\n",
        "import scipy as sp\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 한국어 형태소 및 문장 분석 라이브러리\n",
        "\n",
        "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "!pip install nltk\n",
        "from pykospacing import Spacing\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WoGKBavzWTP2",
        "outputId": "b56e9656-408f-4230-c36b-862c6014538d"
      },
      "source": [
        "df_train[\"title\"].values[1]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'실리콘밸리 넘어서겠다…구글 15조원 들여 美전역 거점화'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlWpXljG3D54",
        "outputId": "de34a4cc-4a47-48ff-bed5-4f57dd7a3366"
      },
      "source": [
        "# train & label data load\n",
        "'''\n",
        "- train_data: index, title, topic_idx\n",
        "- topic_dict: topic, topic_idx\n",
        "- test_data: index, title\n",
        "'''\n",
        "PATH = \"./\"\n",
        "df_train = pd.read_csv(PATH + \"train_data.csv\")\n",
        "df_test = pd.read_csv(PATH + \"test_data.csv\")\n",
        "topic_dict = pd.read_csv(PATH + \"topic_dict.csv\")\n",
        "kr_stopwords_list = pd.read_csv(PATH + \"korean_stopwords.csv\", header = None, names = ['word'])[\"word\"].tolist()\n",
        "\n",
        "# preprocessing\n",
        "# html tag 및 숫자, 영어, 특수문자 등 제거\n",
        "\n",
        "# punctuation remove\n",
        "def clean_punc(text):\n",
        "\n",
        "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&' \n",
        "    punct_mapping = {\n",
        "        \"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \",\n",
        "        \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', \n",
        "        '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha',\n",
        "        '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
        "\n",
        "    for p in punct_mapping:\n",
        "        text = text.replace(p, punct_mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f'{p}')\n",
        "\n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# using re to preprocess: remove punct, html tag, number, and lowering, spacing\n",
        "def clean_text(texts):\n",
        "\n",
        "    corpus = []\n",
        "    for i in range(0, len(texts)):\n",
        "\n",
        "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
        "        review = re.sub(r'\\d+','',str(texts[i])) # remove number\n",
        "        review = review.lower() #lower case\n",
        "        review = re.sub(r'\\s+',' ', review)\n",
        "        review = re.sub(r'<[^>]+>','',review) #remove Html tags \n",
        "        review = re.sub(r'\\s+', ' ', review) #remove spaces \n",
        "        review = re.sub(r\"^\\s+\", '', review) #remove space from start \n",
        "        review = re.sub(r'\\s+$', '', review) #remove space from the end corpus.append(review) return corpus\n",
        "\n",
        "        corpus.append(review)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# dacon sample function\n",
        "def clean_text_dacon(text):\n",
        "    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \" \", text)\n",
        "    return text_clean\n",
        "\n",
        "# 맞춤법, 문장 분리, 단어 분리(외부 라이브러리 이용)\n",
        "    \n",
        "'''\n",
        "\n",
        "try:\n",
        "    from pykospacing import spacing\n",
        "    from hanspell import spell_checker\n",
        "    import kss\n",
        "except:\n",
        "    pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
        "    pip install kss\n",
        "    pip install py-hanspell\n",
        "    from pykospacing import spacing\n",
        "    from hanspell import spell_checker\n",
        "    import kss\n",
        "    \n",
        "'''\n",
        "\n",
        "\n",
        "spacing = Spacing()\n",
        "def text_spacing(text):\n",
        "    text_spacing = spacing(text)\n",
        "    return text_spacing\n",
        "\n",
        "# stemming: 동사를 원형으로 복원\n",
        "# 불용어 제거, nltk\n",
        "# 불용어 제거는 먼저 토큰화가 이루어져야 한다. \n",
        "    \n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def stem_stopwords(text):\n",
        "    corpus = [wnl.lemmatize(word) for word in text.split(' ') if not word in kr_stopwords_list]\n",
        "    return corpus\n",
        "\n",
        "# preprocessing: stopword, spacing, stemming, remove punctuation\n",
        "\n",
        "def preprocessing(df):\n",
        "\n",
        "    df_copy = df.copy()\n",
        "    texts = df_copy[\"title\"].values\n",
        "\n",
        "    text_inputs = []\n",
        "\n",
        "    for text in texts:\n",
        "\n",
        "        text_clean = clean_punc(text)\n",
        "        text_clean = clean_text_dacon(text_clean)\n",
        "        # 시간이 오래 걸리는 관계로 spacing함수를 쓰지 않고 split으로 진행\n",
        "        # text_sc = text_spacing(text_clean)\n",
        "        text_sc = text_clean\n",
        "        corpus_stem_stopwords = stem_stopwords(text_sc)\n",
        "        text_inputs.append(corpus_stem_stopwords)\n",
        "\n",
        "    return text_inputs\n",
        "\n",
        "# tokenized\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "text_preprocessed = preprocessing(df_train)\n",
        "tokenizer = Tokenizer(filters = \" \", num_words = 20000)\n",
        "\n",
        "tokenizer.fit_on_texts(text_preprocessed)\n",
        "datasets_token = tokenizer.texts_to_sequences(text_preprocessed)\n",
        "datasets = pad_sequences(datasets_token, padding = \"post\", dtype = \"int32\")\n",
        "print(datasets.shape)\n",
        "\n",
        "# train - test data split\n",
        "labels = df_train[\"topic_idx\"].values.reshape(-1,).astype('int32')\n",
        "x_train, x_test, y_train, y_test = train_test_split(datasets, labels, test_size = 0.3, shuffle = True, random_state = 42)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45654, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue8zVPggh2_x",
        "outputId": "f9696e6e-71a3-410e-d593-e614b70ea962"
      },
      "source": [
        "print(type(x_train), x_train[0])\n",
        "print(type(y_train), y_train[0])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> [   1    1    1 2526 4164 2160    8    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0]\n",
            "<class 'numpy.ndarray'> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V18kzg5SrDAC",
        "outputId": "706c06f0-9c3d-4f0c-af52-ec2cd45c1fbc"
      },
      "source": [
        ""
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mKkzcjMAcFGt",
        "outputId": "6e4ac7e7-58ea-4723-9b37-524320c061d8"
      },
      "source": [
        "#model parameter\n",
        "\n",
        "vocab_size = tokenizer.num_words\n",
        "maxlen = x_train.shape[1]\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "\n",
        "# Transformer block\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate = 0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
        "        self.ffn = tf.keras.models.Sequential(\n",
        "            [tf.keras.layers.Dense(ff_dim, activation = \"relu\"), tf.keras.layers.Dense(embed_dim)]\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training = True):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = self.maxlen\n",
        "        positions = tf.range(start=0, limit = maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions    \n",
        "\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape = (maxlen,), dtype = \"int32\")\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, rate = 0.2)\n",
        "x = transformer_block(x)\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(7, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name = \"transformer\")\n",
        "model.compile(\n",
        "    #optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics = [\"accuracy\"]\n",
        ")\n",
        "\n",
        "# training\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 64\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 12, verbose = 0)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(\"transformer_weights.h5\",monitor = \"val_loss\", mode = \"min\", patience = 12, save_best_only=True, save_weights_only = True)\n",
        "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.1, patience = 4, mode= \"min\", verbose = 0)\n",
        "\n",
        "callbacks_params = [es, mc, lr]\n",
        "\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "with tf.device(\"gpu:0\"):\n",
        "    hist = model.fit(x_train, y_train, validation_data = (x_test, y_test), \n",
        "                     epochs = epochs, batch_size = batch_size, verbose = 1,\n",
        "                     callbacks = callbacks_params)\n",
        "    loss, acc = hist.history[\"loss\"], hist.history[\"accuracy\"]\n",
        "    epoch_axis = range(1, len(loss) + 1)\n",
        "    plt.plot(epoch_axis, loss, \"r\", label = \"train loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.show() "
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/64\n",
            "250/250 [==============================] - 4s 11ms/step - loss: 1.9321 - accuracy: 0.1827 - val_loss: 1.8855 - val_accuracy: 0.2203\n",
            "Epoch 2/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 1.8795 - accuracy: 0.2295 - val_loss: 1.8355 - val_accuracy: 0.2858\n",
            "Epoch 3/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 1.8170 - accuracy: 0.2886 - val_loss: 1.7353 - val_accuracy: 0.4101\n",
            "Epoch 4/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 1.6870 - accuracy: 0.3815 - val_loss: 1.5540 - val_accuracy: 0.4722\n",
            "Epoch 5/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 1.4714 - accuracy: 0.4831 - val_loss: 1.3345 - val_accuracy: 0.5419\n",
            "Epoch 6/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 1.2481 - accuracy: 0.5605 - val_loss: 1.1110 - val_accuracy: 0.6250\n",
            "Epoch 7/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 1.0752 - accuracy: 0.6245 - val_loss: 1.0170 - val_accuracy: 0.6443\n",
            "Epoch 8/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.9569 - accuracy: 0.6727 - val_loss: 0.9290 - val_accuracy: 0.6809\n",
            "Epoch 9/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.8713 - accuracy: 0.7042 - val_loss: 0.8839 - val_accuracy: 0.6986\n",
            "Epoch 10/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.8125 - accuracy: 0.7248 - val_loss: 0.8560 - val_accuracy: 0.7121\n",
            "Epoch 11/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.7570 - accuracy: 0.7482 - val_loss: 0.8637 - val_accuracy: 0.7069\n",
            "Epoch 12/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.7178 - accuracy: 0.7644 - val_loss: 0.9072 - val_accuracy: 0.6933\n",
            "Epoch 13/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.6838 - accuracy: 0.7766 - val_loss: 0.7905 - val_accuracy: 0.7373\n",
            "Epoch 14/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6505 - accuracy: 0.7894 - val_loss: 0.8681 - val_accuracy: 0.7170\n",
            "Epoch 15/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6261 - accuracy: 0.7993 - val_loss: 0.7882 - val_accuracy: 0.7428\n",
            "Epoch 16/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.6008 - accuracy: 0.8093 - val_loss: 0.7542 - val_accuracy: 0.7546\n",
            "Epoch 17/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5770 - accuracy: 0.8199 - val_loss: 0.7802 - val_accuracy: 0.7524\n",
            "Epoch 18/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5535 - accuracy: 0.8270 - val_loss: 0.7435 - val_accuracy: 0.7600\n",
            "Epoch 19/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5354 - accuracy: 0.8320 - val_loss: 0.7520 - val_accuracy: 0.7628\n",
            "Epoch 20/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5139 - accuracy: 0.8411 - val_loss: 0.7445 - val_accuracy: 0.7664\n",
            "Epoch 21/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.5023 - accuracy: 0.8456 - val_loss: 0.7608 - val_accuracy: 0.7641\n",
            "Epoch 22/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4861 - accuracy: 0.8499 - val_loss: 0.7208 - val_accuracy: 0.7732\n",
            "Epoch 23/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4706 - accuracy: 0.8533 - val_loss: 0.7219 - val_accuracy: 0.7755\n",
            "Epoch 24/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4548 - accuracy: 0.8597 - val_loss: 0.7672 - val_accuracy: 0.7623\n",
            "Epoch 25/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4446 - accuracy: 0.8638 - val_loss: 0.7970 - val_accuracy: 0.7491\n",
            "Epoch 26/64\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.4343 - accuracy: 0.8654 - val_loss: 0.7178 - val_accuracy: 0.7810\n",
            "Epoch 27/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.4234 - accuracy: 0.8701 - val_loss: 0.7282 - val_accuracy: 0.7787\n",
            "Epoch 28/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.4136 - accuracy: 0.8745 - val_loss: 0.7248 - val_accuracy: 0.7791\n",
            "Epoch 29/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.4048 - accuracy: 0.8770 - val_loss: 0.7492 - val_accuracy: 0.7694\n",
            "Epoch 30/64\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.3994 - accuracy: 0.8787 - val_loss: 0.7400 - val_accuracy: 0.7746\n",
            "Epoch 31/64\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.3717 - accuracy: 0.8876 - val_loss: 0.7132 - val_accuracy: 0.7854\n",
            "Epoch 32/64\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.3750 - accuracy: 0.8880 - val_loss: 0.7130 - val_accuracy: 0.7859\n",
            "Epoch 33/64\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.3689 - accuracy: 0.8891 - val_loss: 0.7151 - val_accuracy: 0.7856\n",
            "Epoch 34/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3708 - accuracy: 0.8895 - val_loss: 0.7165 - val_accuracy: 0.7855\n",
            "Epoch 35/64\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 0.3688 - accuracy: 0.8890 - val_loss: 0.7182 - val_accuracy: 0.7846\n",
            "Epoch 36/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3691 - accuracy: 0.8887 - val_loss: 0.7170 - val_accuracy: 0.7860\n",
            "Epoch 37/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3636 - accuracy: 0.8915 - val_loss: 0.7169 - val_accuracy: 0.7862\n",
            "Epoch 38/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3632 - accuracy: 0.8907 - val_loss: 0.7167 - val_accuracy: 0.7863\n",
            "Epoch 39/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3627 - accuracy: 0.8904 - val_loss: 0.7173 - val_accuracy: 0.7870\n",
            "Epoch 40/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3650 - accuracy: 0.8908 - val_loss: 0.7175 - val_accuracy: 0.7854\n",
            "Epoch 41/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3614 - accuracy: 0.8914 - val_loss: 0.7175 - val_accuracy: 0.7859\n",
            "Epoch 42/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3637 - accuracy: 0.8909 - val_loss: 0.7174 - val_accuracy: 0.7859\n",
            "Epoch 43/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3633 - accuracy: 0.8915 - val_loss: 0.7174 - val_accuracy: 0.7859\n",
            "Epoch 44/64\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 0.3637 - accuracy: 0.8914 - val_loss: 0.7174 - val_accuracy: 0.7860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_5JFZpQglICG",
        "outputId": "daf88e76-8fa9-421b-8377-4c48bf4991b4"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# test file -> convert word embedding vector\n",
        "\n",
        "submission_test = preprocessing(df_test)\n",
        "submission_test = tokenizer.texts_to_sequences(submission_test)\n",
        "submission_test = pad_sequences(submission_test, padding = \"post\", dtype = \"int32\", maxlen = maxlen)\n",
        "\n",
        "def softmax_toIdx(predictions):\n",
        "    topic_Idx = np.argmax(predictions, axis = 1).reshape(-1,1)\n",
        "    return topic_Idx\n",
        "\n",
        "# prediction\n",
        "submission_topic = softmax_toIdx(model.predict(submission_test))\n",
        "\n",
        "# submission \n",
        "submission = pd.read_csv(PATH + \"sample_submission.csv\")\n",
        "submission[\"topic_idx\"] = submission_topic\n",
        "submission.to_csv(\"submission.csv\", index = False)\n",
        "files.download(\"submission.csv\")"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0c531a44-6580-41ec-a519-8842e345c373\", \"submission.csv\", 73064)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}